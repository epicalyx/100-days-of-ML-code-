{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network as an XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " ''' Helpful Resource and Credit : https://www.youtube.com/watch?v=vcZub77WvFA&index=6&list=PL2-dafEMk2A7mfQDsEcmxxtxgFEZg0bW-''' "
    "#importing library dependencies\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0,Loss: 0.45465070,Time: 0.0568s\n",
      "Epoch: 1,Loss: 0.13697961,Time: 0.0895s\n",
      "Epoch: 2,Loss: 0.06206941,Time: 0.0518s\n",
      "Epoch: 3,Loss: 0.04092746,Time: 0.0570s\n",
      "Epoch: 4,Loss: 0.03159958,Time: 0.0555s\n",
      "Epoch: 5,Loss: 0.02592744,Time: 0.0625s\n",
      "Epoch: 6,Loss: 0.02199575,Time: 0.0435s\n",
      "Epoch: 7,Loss: 0.01907812,Time: 0.0413s\n",
      "Epoch: 8,Loss: 0.01682099,Time: 0.0506s\n",
      "Epoch: 9,Loss: 0.01502363,Time: 0.0421s\n",
      "Epoch: 10,Loss: 0.01356039,Time: 0.0593s\n",
      "Epoch: 11,Loss: 0.01234775,Time: 0.0425s\n",
      "Epoch: 12,Loss: 0.01132776,Time: 0.0517s\n",
      "Epoch: 13,Loss: 0.01045887,Time: 0.0436s\n",
      "Epoch: 14,Loss: 0.00971052,Time: 0.0463s\n",
      "Epoch: 15,Loss: 0.00905971,Time: 0.0656s\n",
      "Epoch: 16,Loss: 0.00848887,Time: 0.0559s\n",
      "Epoch: 17,Loss: 0.00798436,Time: 0.0477s\n",
      "Epoch: 18,Loss: 0.00753542,Time: 0.0444s\n",
      "Epoch: 19,Loss: 0.00713347,Time: 0.0448s\n",
      "Epoch: 20,Loss: 0.00677160,Time: 0.0748s\n",
      "Epoch: 21,Loss: 0.00644415,Time: 0.0432s\n",
      "Epoch: 22,Loss: 0.00614650,Time: 0.0418s\n",
      "Epoch: 23,Loss: 0.00587477,Time: 0.0443s\n",
      "Epoch: 24,Loss: 0.00562576,Time: 0.0605s\n",
      "Epoch: 25,Loss: 0.00539674,Time: 0.0646s\n",
      "Epoch: 26,Loss: 0.00518541,Time: 0.0410s\n",
      "Epoch: 27,Loss: 0.00498981,Time: 0.0451s\n",
      "Epoch: 28,Loss: 0.00480824,Time: 0.0438s\n",
      "Epoch: 29,Loss: 0.00463926,Time: 0.0413s\n",
      "Epoch: 30,Loss: 0.00448161,Time: 0.0643s\n",
      "Epoch: 31,Loss: 0.00433419,Time: 0.0438s\n",
      "Epoch: 32,Loss: 0.00419603,Time: 0.0617s\n",
      "Epoch: 33,Loss: 0.00406629,Time: 0.0482s\n",
      "Epoch: 34,Loss: 0.00394423,Time: 0.0489s\n",
      "Epoch: 35,Loss: 0.00382919,Time: 0.0865s\n",
      "Epoch: 36,Loss: 0.00372058,Time: 0.0498s\n",
      "Epoch: 37,Loss: 0.00361787,Time: 0.0484s\n",
      "Epoch: 38,Loss: 0.00352061,Time: 0.0461s\n",
      "Epoch: 39,Loss: 0.00342836,Time: 0.0807s\n",
      "Epoch: 40,Loss: 0.00334076,Time: 0.0558s\n",
      "Epoch: 41,Loss: 0.00325746,Time: 0.0469s\n",
      "Epoch: 42,Loss: 0.00317815,Time: 0.0506s\n",
      "Epoch: 43,Loss: 0.00310255,Time: 0.0598s\n",
      "Epoch: 44,Loss: 0.00303042,Time: 0.0597s\n",
      "Epoch: 45,Loss: 0.00296151,Time: 0.0442s\n",
      "Epoch: 46,Loss: 0.00289562,Time: 0.0461s\n",
      "Epoch: 47,Loss: 0.00283255,Time: 0.0455s\n",
      "Epoch: 48,Loss: 0.00277213,Time: 0.0765s\n",
      "Epoch: 49,Loss: 0.00271419,Time: 0.0491s\n",
      "Epoch: 50,Loss: 0.00265858,Time: 0.0445s\n",
      "Epoch: 51,Loss: 0.00260517,Time: 0.0528s\n",
      "Epoch: 52,Loss: 0.00255383,Time: 0.0526s\n",
      "Epoch: 53,Loss: 0.00250444,Time: 0.0633s\n",
      "Epoch: 54,Loss: 0.00245689,Time: 0.0449s\n",
      "Epoch: 55,Loss: 0.00241108,Time: 0.0461s\n",
      "Epoch: 56,Loss: 0.00236692,Time: 0.0514s\n",
      "Epoch: 57,Loss: 0.00232432,Time: 0.0611s\n",
      "Epoch: 58,Loss: 0.00228320,Time: 0.0573s\n",
      "Epoch: 59,Loss: 0.00224348,Time: 0.0514s\n",
      "Epoch: 60,Loss: 0.00220510,Time: 0.0495s\n",
      "Epoch: 61,Loss: 0.00216798,Time: 0.0495s\n",
      "Epoch: 62,Loss: 0.00213207,Time: 0.0641s\n",
      "Epoch: 63,Loss: 0.00209730,Time: 0.0486s\n",
      "Epoch: 64,Loss: 0.00206363,Time: 0.0505s\n",
      "Epoch: 65,Loss: 0.00203101,Time: 0.0456s\n",
      "Epoch: 66,Loss: 0.00199938,Time: 0.0433s\n",
      "Epoch: 67,Loss: 0.00196870,Time: 0.0679s\n",
      "Epoch: 68,Loss: 0.00193892,Time: 0.0541s\n",
      "Epoch: 69,Loss: 0.00191002,Time: 0.0435s\n",
      "Epoch: 70,Loss: 0.00188195,Time: 0.0466s\n",
      "Epoch: 71,Loss: 0.00185467,Time: 0.0502s\n",
      "Epoch: 72,Loss: 0.00182816,Time: 0.1047s\n",
      "Epoch: 73,Loss: 0.00180238,Time: 0.0488s\n",
      "Epoch: 74,Loss: 0.00177730,Time: 0.0505s\n",
      "Epoch: 75,Loss: 0.00175290,Time: 0.1011s\n",
      "Epoch: 76,Loss: 0.00172914,Time: 0.0747s\n",
      "Epoch: 77,Loss: 0.00170600,Time: 0.0776s\n",
      "Epoch: 78,Loss: 0.00168346,Time: 0.0538s\n",
      "Epoch: 79,Loss: 0.00166149,Time: 0.0724s\n",
      "Epoch: 80,Loss: 0.00164008,Time: 0.0617s\n",
      "Epoch: 81,Loss: 0.00161920,Time: 0.0479s\n",
      "Epoch: 82,Loss: 0.00159883,Time: 0.0434s\n",
      "Epoch: 83,Loss: 0.00157896,Time: 0.0775s\n",
      "Epoch: 84,Loss: 0.00155957,Time: 0.0520s\n",
      "Epoch: 85,Loss: 0.00154063,Time: 0.0563s\n",
      "Epoch: 86,Loss: 0.00152214,Time: 0.0518s\n",
      "Epoch: 87,Loss: 0.00150407,Time: 0.0721s\n",
      "Epoch: 88,Loss: 0.00148642,Time: 0.0496s\n",
      "Epoch: 89,Loss: 0.00146917,Time: 0.0475s\n",
      "Epoch: 90,Loss: 0.00145230,Time: 0.0434s\n",
      "Epoch: 91,Loss: 0.00143581,Time: 0.0458s\n",
      "Epoch: 92,Loss: 0.00141968,Time: 0.0786s\n",
      "Epoch: 93,Loss: 0.00140390,Time: 0.0448s\n",
      "Epoch: 94,Loss: 0.00138846,Time: 0.0446s\n",
      "Epoch: 95,Loss: 0.00137334,Time: 0.0598s\n",
      "Epoch: 96,Loss: 0.00135854,Time: 0.0680s\n",
      "Epoch: 97,Loss: 0.00134405,Time: 0.0517s\n",
      "Epoch: 98,Loss: 0.00132986,Time: 0.0422s\n",
      "Epoch: 99,Loss: 0.00131596,Time: 0.0468s\n",
      "XOR Prediction\n",
      "[1 0 1 1 1 1 0 1 0 0]\n",
      "[0 1 0 0 0 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#variables\n",
    "n_hidden_layers=10\n",
    "n_inputs=10\n",
    "#no of output variables\n",
    "n_outputs=10\n",
    "n_samples=300\n",
    "\n",
    "#hyperparameters\n",
    "learning_rate= 0.01\n",
    "momentum= 0.9\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "#activation functions\n",
    "\n",
    "#for output layer\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "#for hidden layer\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "\n",
    "#training model\n",
    "def train(x,t,V,W,bv,bw):\n",
    "    \n",
    "    # forward \n",
    "    A = np.dot(x,V) + bv\n",
    "    Z = np.tanh(A)\n",
    "    \n",
    "    B = np.dot(Z,W) + bw\n",
    "    Y = sigmoid(B)\n",
    "    \n",
    "    \n",
    "    # backward \n",
    "    Ew = Y-t\n",
    "    Ev = tanh_prime(A)*np.dot(W,Ew)\n",
    "    \n",
    "    dW = np.outer(Z,Ew)\n",
    "    dV = np.outer(x,Ev)\n",
    "    \n",
    "    #calculate loss\n",
    "    loss = -np.mean(t*np.log(Y) + (1-t)*np.log(1-Y))\n",
    "    \n",
    "    \n",
    "    return loss,(dV,dW,Ev,Ew)\n",
    "\n",
    "#prediction\n",
    "def predict(x,V,W,bv,bw):\n",
    "    A = np.dot(x,V) + bv\n",
    "    B = np.dot(np.tanh(A),W) + bw\n",
    "    \n",
    "    return (sigmoid(B)>0.5).astype(int)\n",
    "\n",
    "\n",
    "#Setup initial parameters as initialization is important for first order methods\n",
    "\n",
    "V = np.random.normal(scale=0.1,size=(n_inputs,n_hidden_layers))\n",
    "W = np.random.normal(scale=0.1,size=(n_hidden_layers,n_outputs))\n",
    "\n",
    "#initializing biases\n",
    "bv = np.zeros(n_hidden_layers)\n",
    "bw = np.zeros(n_outputs)\n",
    "\n",
    "params = [V,W,bv,bw]\n",
    "\n",
    "#Generate some data\n",
    "\n",
    "X = np.random.binomial(1,0.5,(n_samples,n_inputs))\n",
    "T = X^1\n",
    "\n",
    "#Train\n",
    "for epoch in range(100):\n",
    "    err = []\n",
    "    upd = [0]*len(params)\n",
    "    \n",
    "    t0 = time.clock()\n",
    "    for i in range(X.shape[0]):\n",
    "        loss,grad = train(X[i],T[i],*params)\n",
    "        \n",
    "        for j in range(len(params)):\n",
    "            params[j]-= upd[j]\n",
    "            \n",
    "        for j in range(len(params)):\n",
    "            upd[j] = learning_rate*grad[j] + momentum*upd[j]\n",
    "            \n",
    "        err.append(loss)\n",
    "        \n",
    "    print(\"Epoch: %d,Loss: %.8f,Time: %.4fs\" %(epoch,np.mean(err),time.clock()-t0))\n",
    "    \n",
    "    \n",
    "# Try to predict something\n",
    "x=np.random.binomial(1,0.5,n_inputs)\n",
    "print(\"XOR Prediction\")\n",
    "print(x)\n",
    "print(predict(x,*params))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
